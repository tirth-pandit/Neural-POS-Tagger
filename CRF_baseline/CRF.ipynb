{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nltk.tag import CRFTagger\n",
    "\n",
    "from stemmer import Stemmer\n",
    "from tokenizer import WordTokenizer\n",
    "from alphabet import numbers, punctuations\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traindata(path):\n",
    "    \n",
    "    f = open(path)\n",
    "    \n",
    "    lines= f.readlines()\n",
    "    \n",
    "    i=0\n",
    "    data = []\n",
    "    while i < len(lines):\n",
    "        temp = lines[i].split()\n",
    "\n",
    "        stmt = []\n",
    "        if temp[0] == '<s>':\n",
    "            isend = i+1\n",
    "            i+=1\n",
    "            \n",
    "            while lines[isend].split()[0] != '</s>':\n",
    "                t = lines[isend].split()\n",
    "                stmt.append( tuple(t))\n",
    "                isend +=1\n",
    "                i+=1\n",
    "                \n",
    "        \n",
    "        i+=1 \n",
    "        data.append(stmt)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat(idx , tokens ):\n",
    "\n",
    "    stemmer = Stemmer()\n",
    "    numbs = numbers.values()\n",
    "    puncts = punctuations.values()\n",
    "\n",
    "    token = stemmer.stem(tokens[idx])\n",
    "    feature_list = []\n",
    "\n",
    "    if not token:\n",
    "        return feature_list\n",
    "\n",
    "    for number in numbs:\n",
    "        if number in list(token):\n",
    "            feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "    for punctuation in puncts:\n",
    "        if punctuation in list(token):\n",
    "            feature_list.append(\"PUNCTUATION\")\n",
    "\n",
    "    feature_list.append(\"WORD_\" + token)\n",
    "\n",
    "    if len(token) > 1:\n",
    "        feature_list.append(\"SUF_\" + token[-1:])\n",
    "        feature_list.append(\"PRE_\" + token[:1])\n",
    "    if len(token) > 2:\n",
    "        feature_list.append(\"SUF_\" + token[-2:])\n",
    "        feature_list.append(\"PRE_\" + token[:2])\n",
    "    if len(token) > 3:\n",
    "        feature_list.append(\"SUF_\" + token[-3:])\n",
    "        feature_list.append(\"PRE_\" + token[:3])\n",
    "\n",
    "    if idx >= 1:\n",
    "        previous_token = stemmer.stem(tokens[idx-1])\n",
    "        if not previous_token:\n",
    "            return feature_list\n",
    "\n",
    "        for number in numbs:\n",
    "            if number in list(previous_token):\n",
    "                feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "        for punctuation in puncts:\n",
    "            if punctuation in list(previous_token):\n",
    "                feature_list.append(\"PUNCTUATION\")\n",
    "\n",
    "        if len(previous_token) > 1:\n",
    "            feature_list.append(\"SUF_\" + previous_token[-1:])\n",
    "            feature_list.append(\"PRE_\" + previous_token[:1])\n",
    "        if len(previous_token) > 2:\n",
    "            feature_list.append(\"SUF_\" + previous_token[-2:])\n",
    "            feature_list.append(\"PRE_\" + previous_token[:2])\n",
    "        if len(previous_token) > 3:\n",
    "            feature_list.append(\"SUF_\" + previous_token[-3:])\n",
    "            feature_list.append(\"PRE_\" + previous_token[:3])\n",
    "\n",
    "        feature_list.append(\"PREV_WORD_\" + previous_token)\n",
    "\n",
    "    if idx >= 2:\n",
    "        previous_token = stemmer.stem(tokens[idx-2])\n",
    "        if not previous_token:\n",
    "            return feature_list\n",
    "\n",
    "        for number in numbs:\n",
    "            if number in list(previous_token):\n",
    "                feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "        for punctuation in puncts:\n",
    "            if punctuation in list(previous_token):\n",
    "                feature_list.append(\"PUNCTUATION\")\n",
    "\n",
    "        if len(previous_token) > 1:\n",
    "            feature_list.append(\"SUF_\" + previous_token[-1:])\n",
    "            feature_list.append(\"PRE_\" + previous_token[:1])\n",
    "        if len(previous_token) > 2:\n",
    "            feature_list.append(\"SUF_\" + previous_token[-2:])\n",
    "            feature_list.append(\"PRE_\" + previous_token[:2])\n",
    "        if len(previous_token) > 3:\n",
    "            feature_list.append(\"SUF_\" + previous_token[-3:])\n",
    "            feature_list.append(\"PRE_\" + previous_token[:3])\n",
    "\n",
    "        feature_list.append(\"PREV_PREV_WORD_\" + previous_token)\n",
    "\n",
    "\n",
    "\n",
    "    if idx < len(tokens)-1:\n",
    "        next_token = stemmer.stem(tokens[idx+1])\n",
    "        if not next_token:\n",
    "            return feature_list\n",
    "\n",
    "        for number in numbs:\n",
    "            if number in list(next_token):\n",
    "                feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "        for punctuation in puncts:\n",
    "            if punctuation in list(next_token):\n",
    "                feature_list.append(\"PUNCTUATION\")\n",
    "\n",
    "        if len(next_token) > 1:\n",
    "            feature_list.append(\"SUF_\" + next_token[-1:])\n",
    "            feature_list.append(\"PRE_\" + next_token[:1])\n",
    "        if len(next_token) > 2:\n",
    "            feature_list.append(\"SUF_\" + next_token[-2:])\n",
    "            feature_list.append(\"PRE_\" + next_token[:2])\n",
    "        if len(next_token) > 3:\n",
    "            feature_list.append(\"SUF_\" + next_token[-3:])\n",
    "            feature_list.append(\"PRE_\" + next_token[:3])\n",
    "\n",
    "        feature_list.append(\"NEXT_WORD_\" + next_token)\n",
    "\n",
    "    if idx < len(tokens)-2:\n",
    "        next_token = stemmer.stem(tokens[idx+2])\n",
    "        if not next_token:\n",
    "            return feature_list\n",
    "\n",
    "        for number in numbs:\n",
    "            if number in list(next_token):\n",
    "                feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "        for punctuation in puncts:\n",
    "            if punctuation in list(next_token):\n",
    "                feature_list.append(\"PUNCTUATION\")\n",
    "\n",
    "        if len(next_token) > 1:\n",
    "            feature_list.append(\"SUF_\" + next_token[-1:])\n",
    "            feature_list.append(\"PRE_\" + next_token[:1])\n",
    "        if len(next_token) > 2:\n",
    "            feature_list.append(\"SUF_\" + next_token[-2:])\n",
    "            feature_list.append(\"PRE_\" + next_token[:2])\n",
    "        if len(next_token) > 3:\n",
    "            feature_list.append(\"SUF_\" + next_token[-3:])\n",
    "            feature_list.append(\"PRE_\" + next_token[:3])\n",
    "\n",
    "        feature_list.append(\"NEXT_NEXT_WORD_\" + next_token)\n",
    "\n",
    "    return feature_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(tokens):\n",
    "\n",
    "    for stmt in tokens:\n",
    "\n",
    "        for i in range(len(stmt)):\n",
    "\n",
    "            feature = feat(i ,stmt)\n",
    "            print( stmt[i] )\n",
    "            print(feature)\n",
    "            break\n",
    "        break\n",
    "\n",
    "\n",
    "def pos_tag(sentence):\n",
    "    stemmer = Stemmer()\n",
    "    sent = stemmer.stem(sentence)\n",
    "    tokens = WordTokenizer(sent)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tok(data):\n",
    "    stemmer = Stemmer()\n",
    "    new_train = []\n",
    "    \n",
    "    for stmt in data :\n",
    "        temp = []\n",
    "\n",
    "        for word in stmt :\n",
    "\n",
    "            sent = stemmer.stem(word[0])\n",
    "            toks = WordTokenizer(sent)\n",
    "\n",
    "            if len(toks) == 0:\n",
    "                continue\n",
    "\n",
    "            temp.append(tuple( [toks[0] ,word][1] ))\n",
    "        new_train.append(temp)\n",
    "    \n",
    "    return new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_path ):\n",
    "    train = get_traindata(train_path)\n",
    "    \n",
    "    train = stem_tok(train)\n",
    "\n",
    "    ct = CRFTagger()\n",
    "    ct.train(train,'model.crf.tagger')\n",
    "    \n",
    "    return ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokenize(data ,tag):\n",
    "    stemmer = Stemmer()\n",
    "    new_str = []\n",
    "    new_tags = []\n",
    "    \n",
    "    for i in range(len(data)) :\n",
    "        sent = stemmer.stem(data[i])\n",
    "        toks = WordTokenizer(sent)\n",
    "\n",
    "        if len(toks) == 0:\n",
    "            continue\n",
    "\n",
    "        new_str.append(toks[0])\n",
    "        new_tags.append(tag[i])\n",
    "    \n",
    "    return new_str ,new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testdata(path):\n",
    "    \n",
    "    f = open(path)\n",
    "    \n",
    "    lines= f.readlines()\n",
    "    \n",
    "    test = []\n",
    "    tags = []\n",
    "    \n",
    "    for stmt in lines:\n",
    "        temp = stmt.split()\n",
    "                \n",
    "        string = []\n",
    "        tag = []\n",
    "        \n",
    "        for j in range(len(temp)):\n",
    "            \n",
    "            t = temp[j].split('_')\n",
    "\n",
    "            if j!=len(temp)-1:\n",
    "                string.append(t[0])\n",
    "                \n",
    "                if len(t) == 2:\n",
    "                    tag.append(t[1])\n",
    "                else:\n",
    "                    tag.append(t[1]+'_'+t[2])\n",
    "\n",
    "        \n",
    "        string ,tag = stem_tokenize(string ,tag)\n",
    "        test.append(string)\n",
    "        tags.append(tag)\n",
    "\n",
    "    \n",
    "    return test , tags\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(data):\n",
    "    pred = []\n",
    "    \n",
    "    for i in data:\n",
    "        temp= []\n",
    "        for j in i:\n",
    "             temp.append(j[1])\n",
    "        \n",
    "        pred.append(temp)\n",
    "        \n",
    "    return pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fun(model ,test_path):\n",
    "    \n",
    "    test,labels = get_testdata(test_path)\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for i in test:\n",
    "        output.append(model.tag(i))\n",
    "    \n",
    "    pred_tags = get_tags(output)\n",
    "    \n",
    "    label = []\n",
    "    predict = []\n",
    "    for i in range(len(labels)):\n",
    "        for j in range( len(labels[i])):\n",
    "\n",
    "            label.append(labels[i][j])\n",
    "            predict.append(pred_tags[i][j])\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "\n",
    "    full = label + predict\n",
    "    le.fit(full)\n",
    "\n",
    "    labels = le.transform(label)\n",
    "    preds = le.transform(predict)\n",
    "    \n",
    "    print('Accuracy : '+ str(accuracy_score(labels ,preds)))\n",
    "    print('F1 Score : '+ str(f1_score(labels ,preds ,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Art and Culture\n",
      "Accuracy : 0.685150955021565\n",
      "F1 Score : 0.4594274470259343\n"
     ]
    }
   ],
   "source": [
    "print('Art and Culture')\n",
    "model = train('train_data/train_guj_art and culture_sample1.txt')\n",
    "test_fun(model ,'test_data/guj_art and culture_sample1_tags.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Economy\n",
      "Accuracy : 0.734910277324633\n",
      "F1 Score : 0.5310406967351758\n"
     ]
    }
   ],
   "source": [
    "print('Economy')\n",
    "model = train('train_data/train_guj_economy_sample2.txt')\n",
    "test_fun(model ,'test_data/guj_economy_sample2_tags.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entertainment\n",
      "Accuracy : 0.6373008434864105\n",
      "F1 Score : 0.4300666018153045\n"
     ]
    }
   ],
   "source": [
    "print('Entertainment')\n",
    "model = train('train_data/train_guj_entertainment_sample3.txt')\n",
    "test_fun(model ,'test_data/guj_entertainment_sample3_tags.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Philosophy\n",
      "Accuracy : 0.7438345266507558\n",
      "F1 Score : 0.5563661500554326\n"
     ]
    }
   ],
   "source": [
    "print('Philosophy')\n",
    "model = train('train_data/train_guj_philosophy_sample4.txt')\n",
    "test_fun(model ,'test_data/guj_philosophy_sample4_tags.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Religion\n",
      "Accuracy : 0.6538461538461539\n",
      "F1 Score : 0.4877179531472362\n"
     ]
    }
   ],
   "source": [
    "print('Religion')\n",
    "model = train('train_data/train_guj_religion_sample5.txt')\n",
    "test_fun(model ,'test_data/guj_religion_sample5_tags.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Science And Tech\n",
      "Accuracy : 0.6975512905360688\n",
      "F1 Score : 0.49164867291251735\n"
     ]
    }
   ],
   "source": [
    "print('Science And Tech')\n",
    "model = train('train_data/train_guj_science and technology_sample6.txt')\n",
    "test_fun(model ,'test_data/guj_science and technology_sample6_tags.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sports\n",
      "Accuracy : 0.6161971830985915\n",
      "F1 Score : 0.4914905327674904\n"
     ]
    }
   ],
   "source": [
    "print('Sports')\n",
    "model = train('train_data/train_guj_sports_sample7.txt')\n",
    "test_fun(model ,'test_data/guj_sports_sample7_tags.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
